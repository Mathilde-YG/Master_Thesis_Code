{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c712d5c4-5595-4588-85fe-f54e0c99e3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/lib/python3.12/site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from prawcore<3,>=2.4->praw) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a962efe9-17ba-4034-8ed4-b7b617d995de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asyncpraw in /opt/anaconda3/lib/python3.12/site-packages (7.8.1)\n",
      "Requirement already satisfied: aiofiles in /opt/anaconda3/lib/python3.12/site-packages (from asyncpraw) (24.1.0)\n",
      "Requirement already satisfied: aiohttp<4 in /opt/anaconda3/lib/python3.12/site-packages (from asyncpraw) (3.9.5)\n",
      "Requirement already satisfied: aiosqlite<=0.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from asyncpraw) (0.17.0)\n",
      "Requirement already satisfied: asyncprawcore<3,>=2.4 in /opt/anaconda3/lib/python3.12/site-packages (from asyncpraw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /opt/anaconda3/lib/python3.12/site-packages (from asyncpraw) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4->asyncpraw) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4->asyncpraw) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4->asyncpraw) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4->asyncpraw) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4->asyncpraw) (1.18.0)\n",
      "Requirement already satisfied: typing_extensions>=3.7.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiosqlite<=0.17.0->asyncpraw) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp<4->asyncpraw) (2.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp<4->asyncpraw) (0.2.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from update_checker>=0.18->asyncpraw) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.3.0->update_checker>=0.18->asyncpraw) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install asyncpraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80da5a7a-37e6-4dae-9a39-85b5b6f72bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /opt/anaconda3/lib/python3.12/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5815b0ec-f5c3-40f5-bc99-4b24afbe7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncpraw\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "nest_asyncio.apply()  # Required for Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da15db5d-f3c8-4504-b979-03a1c7ac487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joyeongyeong/Documents/Thesis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Reddit_Data_Scrap.ipynb',\n",
       " '2_filtered_products_personal.jsonl',\n",
       " '4_Final_matched_review_products_personal.jsonl',\n",
       " '.DS_Store',\n",
       " '3_Visualization.ipynb',\n",
       " '2_filtered_products_beauty.jsonl',\n",
       " 'Preprocessing_reddit.ipynb',\n",
       " '3_matched_review_products_beauty.jsonl',\n",
       " 'Beauty.jsonl',\n",
       " '4_Final_matched_review_products_beauty.jsonl',\n",
       " 'amazon-review-scraper',\n",
       " '3_matched_review_products_personal.jsonl',\n",
       " 'Personal_Care.jsonl',\n",
       " 'Amazon_2023_updated.ipynb',\n",
       " 'meta_Personal_Care.jsonl',\n",
       " 'meta_All_Beauty.jsonl',\n",
       " '.ipynb_checkpoints',\n",
       " 'Reddit_scrap.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # 현재 작업 디렉토리 확인\n",
    "os.listdir()        # 현재 디렉토리 내 파일 목록 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b02e252-6b60-441a-96d1-58d23d320b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering fetch posts with la roche-posay..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with la roche-posay..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with la roche-posay..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with la roche-posay..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with la roche-posay..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with la roche-posay..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with laroche..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with laroche..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with laroche..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with laroche..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with laroche..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with laroche..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with effaclar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with effaclar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with effaclar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with effaclar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with effaclar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with effaclar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with toleriane..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with toleriane..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with toleriane..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with toleriane..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with toleriane..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with toleriane..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_niche_la_roche-posay.csv (10051 rows)\n",
      "Entering fetch posts with avène..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avène..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avène..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avène..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avène..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avène..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avene..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avene..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avene..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avene..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avene..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with avene..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with thermal spring..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with thermal spring..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with thermal spring..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with thermal spring..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with thermal spring..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with thermal spring..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cicalfate..\n",
      "await subreddit\n",
      "subreddit search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n",
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Entering fetch posts with cicalfate..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cicalfate..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cicalfate..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cicalfate..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cicalfate..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_niche_avène.csv (6060 rows)\n",
      "Entering fetch posts with bioderma..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bioderma..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bioderma..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bioderma..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bioderma..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bioderma..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with sensibio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with sensibio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with sensibio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with sensibio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with sensibio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with sensibio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydrabio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydrabio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydrabio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydrabio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydrabio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydrabio..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_niche_bioderma.csv (5242 rows)\n",
      "Entering fetch posts with cerave..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave cleanser..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave cleanser..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave cleanser..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave cleanser..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave cleanser..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave cleanser..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave moisturizer..\n",
      "await subreddit\n",
      "subreddit search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n",
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Entering fetch posts with cerave moisturizer..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave moisturizer..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave moisturizer..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave moisturizer..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with cerave moisturizer..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_niche_cerave.csv (18268 rows)\n",
      "Entering fetch posts with uriage..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with uriage..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with uriage..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with uriage..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with uriage..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with uriage..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bariederm..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bariederm..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bariederm..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bariederm..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bariederm..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with bariederm..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with eau thermale..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with eau thermale..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with eau thermale..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with eau thermale..\n",
      "await subreddit\n",
      "subreddit search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n",
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Entering fetch posts with eau thermale..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with eau thermale..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_niche_uriage.csv (3304 rows)\n",
      "Entering fetch posts with garnier..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with garnier..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with garnier..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with garnier..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with garnier..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with garnier..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with micellar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with micellar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with micellar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with micellar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with micellar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with micellar..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with skinactive..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with skinactive..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with skinactive..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with skinactive..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with skinactive..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with skinactive..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_mass_garnier.csv (12308 rows)\n",
      "Entering fetch posts with l’oréal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with l’oréal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with l’oréal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with l’oréal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with l’oréal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with l’oréal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with loreal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with loreal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with loreal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with loreal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with loreal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with loreal..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with revitalift..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with revitalift..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with revitalift..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with revitalift..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with revitalift..\n",
      "await subreddit\n",
      "subreddit search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n",
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Entering fetch posts with revitalift..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydra genius..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydra genius..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydra genius..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydra genius..\n",
      "await subreddit\n",
      "subreddit search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n",
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Entering fetch posts with hydra genius..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydra genius..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_mass_loreal_paris.csv (7975 rows)\n",
      "Entering fetch posts with neutrogena..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with neutrogena..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with neutrogena..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with neutrogena..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with neutrogena..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with neutrogena..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydro boost..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydro boost..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydro boost..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydro boost..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydro boost..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with hydro boost..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with rainbath..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with rainbath..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with rainbath..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with rainbath..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with rainbath..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with rainbath..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_mass_neutrogena.csv (9664 rows)\n",
      "Entering fetch posts with nivea..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with nivea..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with nivea..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with nivea..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with nivea..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with nivea..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with soft cream..\n",
      "await subreddit\n",
      "subreddit search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n",
      "/var/folders/bj/7qq4f8jd6c5cghfq94hrdlhr0000gn/T/ipykernel_8167/428290032.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_data = pd.concat([all_data, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Entering fetch posts with soft cream..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with soft cream..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with soft cream..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with soft cream..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with soft cream..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with q10..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with q10..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with q10..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with q10..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with q10..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Entering fetch posts with q10..\n",
      "await subreddit\n",
      "subreddit search\n",
      "DONE\n",
      "Saved: reddit_mass_nivea.csv (11286 rows)\n"
     ]
    }
   ],
   "source": [
    "# Brand keywords mapping\n",
    "brand_keywords_map = {\n",
    "    \"La Roche-Posay\": [\"la roche-posay\", \"laroche\", \"effaclar\", \"toleriane\"],\n",
    "    \"Avène\": [\"avène\", \"avene\", \"thermal spring\", \"cicalfate\"],\n",
    "    \"Bioderma\": [\"bioderma\", \"sensibio\", \"hydrabio\"],\n",
    "    \"CeraVe\": [\"cerave\", \"cerave cleanser\", \"cerave moisturizer\"],\n",
    "    \"Uriage\": [\"uriage\", \"bariederm\", \"eau thermale\"],\n",
    "    \"Garnier\": [\"garnier\", \"micellar\", \"skinactive\"],\n",
    "    \"L’Oréal Paris\": [\"l’oréal\", \"loreal\", \"revitalift\", \"hydra genius\"],\n",
    "    \"Neutrogena\": [\"neutrogena\", \"hydro boost\", \"rainbath\"],\n",
    "    \"Nivea\": [\"nivea\", \"soft cream\", \"q10\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Brand Position Mapping\n",
    "brand_positioning_map = {\n",
    "    \"La Roche-Posay\": \"niche\",\n",
    "    \"Avène\": \"niche\",\n",
    "    \"Bioderma\": \"niche\",\n",
    "    \"CeraVe\": \"niche\",\n",
    "    \"Uriage\": \"niche\",\n",
    "    \"Garnier\": \"mass\",\n",
    "    \"L’Oréal Paris\": \"mass\",\n",
    "    \"Neutrogena\": \"mass\",\n",
    "    \"Nivea\": \"mass\",\n",
    "}\n",
    "\n",
    "\n",
    "# data period\n",
    "start_date = datetime(2019, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "async def fetch_posts_with_keyword(subreddit_name, keyword, positioning, brand, limit=1000):\n",
    "    reddit = asyncpraw.Reddit(\n",
    "        client_id=\"OQ7pfL7IImXBD1LlNsWUdw\",\n",
    "        client_secret=\"6povn4KpOaXDSTsWpQClHo2kBhDlSg\",\n",
    "        user_agent=\"DataCollect\"\n",
    "    )\n",
    "    merged_rows = []\n",
    "    try:\n",
    "        print(f\"await subreddit\")\n",
    "        subreddit = await reddit.subreddit(subreddit_name)\n",
    "        print(f\"subreddit search\")\n",
    "        async for post in subreddit.search(keyword, sort='relevance', time_filter='all', limit=limit):\n",
    "            try:\n",
    "                post_date = datetime.fromtimestamp(post.created)\n",
    "                if start_date <= post_date <= end_date:                 # Date filtering\n",
    "                    await post.load()\n",
    "                    # Filtering short text\n",
    "                    if post.selftext and len(post.selftext.split()) < 5:\n",
    "                        continue\n",
    "                    post_row = {\n",
    "                        \"positioning\": positioning,\n",
    "                        \"brand\": brand,\n",
    "                        \"p_title\": post.title,\n",
    "                        \"p_score\": post.score,\n",
    "                        \"num_comments\": post.num_comments,\n",
    "                        \"p_text\": post.selftext,\n",
    "                        \"p_created_date\": post_date,\n",
    "                        \"c_text\": None,\n",
    "                        \"c_score\": None,\n",
    "                        \"c_created_date\": None\n",
    "                    }\n",
    "                    merged_rows.append(post_row)\n",
    "                    for comment in post.comments:\n",
    "                        if isinstance(comment, asyncpraw.models.Comment):\n",
    "                            if comment.body and len(comment.body.split()) < 5:\n",
    "                                continue\n",
    "                            comment_row = {\n",
    "                                \"positioning\": positioning,\n",
    "                                \"brand\": brand,\n",
    "                                \"p_title\": post.title,\n",
    "                                \"p_score\": post.score,\n",
    "                                \"num_comments\": post.num_comments,\n",
    "                                \"p_text\": post.selftext,\n",
    "                                \"p_created_date\": post_date,\n",
    "                                \"c_text\": comment.body,\n",
    "                                \"c_score\": comment.score,\n",
    "                                \"c_created_date\": datetime.fromtimestamp(comment.created)\n",
    "                            }\n",
    "                            merged_rows.append(comment_row)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    finally:\n",
    "        await reddit.close()\n",
    "    # Delete repetition\n",
    "    df = pd.DataFrame(merged_rows).drop_duplicates(subset=[\"p_title\", \"p_text\", \"c_text\"])\n",
    "    return df\n",
    "\n",
    "async def main():\n",
    "    subreddits = [\n",
    "        \"SkincareAddiction\", \"EuroSkincare\", \"beauty\",\n",
    "        \"AsianBeauty\", \"MakeupAddiction\", \"SkincareScience\"\n",
    "    ]\n",
    "    min_samples = 500  # the minimum sample per brand\n",
    "    for brand, positioning in brand_positioning_map.items():\n",
    "        all_data = pd.DataFrame()\n",
    "        for keyword in brand_keywords_map[brand]:\n",
    "            for subreddit in subreddits:\n",
    "                print(f\"Entering fetch posts with {keyword}..\")\n",
    "                df = await fetch_posts_with_keyword(subreddit, keyword, positioning, brand, limit=500)\n",
    "                print(f\"DONE\")\n",
    "                all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        # Delete Null/Duplications\n",
    "        all_data = all_data.drop_duplicates(subset=[\"p_title\", \"p_text\", \"c_text\"])\n",
    "        all_data = all_data.dropna(subset=[\"p_text\"])\n",
    "        # Warning\n",
    "        if len(all_data) < min_samples:\n",
    "            print(f\"[Warining] {brand} Data isn't sufficient: {len(all_data)}\")\n",
    "        file_name = f\"reddit_{positioning.lower()}_{brand.lower().replace(' ', '_').replace('’', '').replace('é', 'e')}.csv\"\n",
    "        all_data.to_csv(file_name, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Saved: {file_name} ({len(all_data)} rows)\")\n",
    "\n",
    "# Running\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e40e2-d053-436f-bfce-44bbc8076f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b1df5-ca06-4c35-8453-e135cdd87937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8107f-7233-486e-944f-050dbdd3b814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your_env_name)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
